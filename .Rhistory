#Common words only
common_words_all <- cleaned_freq_all[ , colSums(1*(cleaned_freq_all > 0)) > (num_all*common_level)]
#Frequent and common words only (Core words)
core_words_all <- cleaned_freq_all[ , (colSums(1*(cleaned_freq_all > 0)) > (num_all*common_level)) & (colSums(cleaned_freq_all) > (sum(colSums(cleaned_freq_all))*freq_level))]
#Turn core words into a dataframe including commonality and frequency
core_words_all_chart <- data.frame('Composite frequency' = colSums(core_words_all), 'Relative frequency per 1000 words' =  colSums(core_words_all)/total_words_all*1000, 'Commonality' = colSums(1*(core_words_all > 0))/num_all)
#Order by frequency
ordered_core_words_all <- core_words_all_chart[order(core_words_all_chart[ ,1], decreasing = TRUE), ]
gtall <- gt(round(ordered_core_words_all[1:6, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gtall, title = "Preview of Core Words Across All Ages")
gt4 <- gt(round(ordered_core_words_4[1:3, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gt4, title = "Preview of Core Words for 4-Year-Olds")
gt5 <- gt(round(ordered_core_words_5[1:3, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gt5, title = "Preview of Core Words for 5-Year-Olds")
gt6 <- gt(round(ordered_core_words_6[1:3, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gt6, title = "Preview of Core Words for 6-Year-Olds")
#creating a smoothing spline with 7 degrees of freedom
smooth.model <- smooth.spline(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency, df = 7)
#Plotting using plot function
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(smooth.model)
#Turning output of smoothing spline into a data frame
smoothing_dataframe <- data.frame(predict(smooth.model))
#Creating the basic ggplot
smoothing_plot <- ggplot(data = ordered_core_words_all, mapping = aes(x = Commonality, y = Composite.frequency)) +
geom_point() +
geom_line(data = smoothing_dataframe, mapping = aes(x = x, y = y))
#Customizing and printing
smoothing_plot +
theme_bw() +
theme(plot.caption = element_text(hjust = 0)) +
labs(y = "Frequency",
caption = "A smoothing spline is fit to the relationship of commonality and total frequency of core words in the combined \ngroup of children.")
#Creating a table with notice of core words & commonality for later
length_of_vects <- length(colnames(cleaned_freq_all))
vec_core_4 <- vector(mode = "character", length = length_of_vects)
vec_core_5 <- vector(mode = "character", length = length_of_vects)
vec_core_6 <- vector(mode = "character", length = length_of_vects)
running_count <- vector(mode = "numeric", length = length_of_vects)
ii <- 0
for (name in colnames(cleaned_freq_all)) {
ii <- ii + 1
ifelse(any(row.names(core_words_4_chart) == name), vec_core_4[ii] <- "x", vec_core_4[ii] <- "")
ifelse(any(row.names(core_words_5_chart) == name), vec_core_5[ii] <- "x", vec_core_5[ii] <- "")
ifelse(any(row.names(core_words_6_chart) == name), vec_core_6[ii] <- "x", vec_core_6[ii] <- "")
}
total_frequency <- colSums(cleaned_freq_all)
total_commonality <- colSums(1*(cleaned_freq_all > 0))
core_words_check <- data.frame('4 Year Olds' = vec_core_4, '5 Year Olds' = vec_core_5, '6 Year Olds' = vec_core_6, 'Composite frequency' = colSums(cleaned_freq_all), 'Relative frequency per 1000 words' =  colSums(cleaned_freq_all)/(sum(colSums(cleaned_freq_all)))*1000, 'Commonality' = colSums(1*(cleaned_freq_all > 0))/(num_all))
row.names(core_words_check) <- colnames(cleaned_freq_all)
core_words_check <- core_words_check[order(core_words_check$Composite.frequency, decreasing = TRUE), ]
check_if_any_core <- function(x){
any(x == "x")
}
core_words_check_noblanks <- core_words_check[apply(core_words_check, MARGIN = 1, check_if_any_core), ]
gtcore <- gt(cbind(core_words_check_noblanks[1:6,1:3 ],round(core_words_check_noblanks[1:6, 4:6], digits = 2)), rownames_to_stub = TRUE)
tab_header(gtcore, title = "Preview of Core Words for Different Age Groups")
#Smoothing the scatterplot
#Averaging
averaged_common <- aggregate(Composite.frequency~Commonality, data = ordered_core_words_all, FUN = mean)
#Simple plot
#plot(averaged_common$Commonality, averaged_common$Composite.frequency)
#Creating ggplot
averaged_scatter_plot <- ggplot(data = averaged_common, mapping = aes(x = Commonality, y = Composite.frequency)) +
geom_point()
#Customizing and printing
averaged_scatter_plot +
theme_bw() +
theme(plot.caption = element_text(hjust = 0)) +
labs(y = "Frequency",
caption = "A smoothed scatterplot created by averaging the frequency of core words with the same commonality in the \ncombined group of children.")
#Dataframe with additional transformed variables
expanded_chart <- cbind(core_words_all_chart, log.freq = log(core_words_all_chart[ ,1]), com2 <- (core_words_all_chart[ ,3])^2, com3 <- (core_words_all_chart[ ,3])^3, com4 <- (core_words_all_chart[ ,3])^4)
#exponential model
exp.model <- lm(log.freq ~ Commonality, data = expanded_chart)
#Points on exponential model
exp.points <- data.frame(Commonality = seq(.3, 1, by = .05), Composite.frequency = exp(exp.model$coefficients[1]*seq(.3, 1, by = .05) + exp.model$coefficients[2]))
#simple plot
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(seq(.3, 1, by=.05), exp(exp.model$coefficients[1]*seq(.3, 1, by=.05) + exp.model$coefficients[2]))
#Creating ggplot
exp_plot <- ggplot(data = ordered_core_words_all, mapping = aes(x = Commonality, y = Composite.frequency)) +
geom_point() +
geom_line(data = exp.points, mapping = aes(x = Commonality, y = Composite.frequency))
#Customizing and printing
exp_plot +
theme_bw() +
theme(plot.caption = element_text(hjust = 0)) +
labs(y = "Frequency",
caption = "An exponential model is fit to the relationship of commonality and total frequency of core words in the combined \ngroup of children.")
#best determined threshold for very common words
chop.point <- .98
#splitting of data
small_comm <- ordered_core_words_all[ordered_core_words_all[,3] < chop.point, ]
small_comm_log <- cbind(small_comm, log.freq = log(small_comm$Composite.frequency))
large_comm <- ordered_core_words_all[ordered_core_words_all[,3] >= chop.point, ]
#fitting of each line
lower.line <- lm(log.freq ~ Commonality, data = small_comm_log)
upper.point <- mean(large_comm$Composite.frequency)
#calculation of total squared loss
sqloss <- sum((small_comm$Composite.frequency - exp(lower.line$coefficients[1]*small_comm$Commonality + lower.line$coefficients[2]))^2) + sum((large_comm$Composite.frequency - upper.point)^2)
#simple plot
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(seq(.3, chop.point, by=.01), exp(lower.line$coefficients[1]*seq(.3, chop.point, by=.01) + lower.line$coefficients[2]))
#lines(seq(chop.point, 1, by=.01), seq(chop.point, 1, by=.01)*0+upper.point)
#Points on exponential model
left.points <- data.frame(Commonality = seq(.3, chop.point, by = .01), Composite.frequency = exp(lower.line$coefficients[1]*seq(.3, chop.point, by = .01) + lower.line$coefficients[2]))
right.points <- data.frame(Commonality = seq(chop.point, 1, by = .01), Composite.frequency = seq(chop.point, 1, by = .01)*0 + upper.point)
#simple plot
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(seq(.3, 1, by=.05), exp(exp.model$coefficients[1]*seq(.3, 1, by=.05) + exp.model$coefficients[2]))
#Creating ggplot
pw_plot <- ggplot(data = ordered_core_words_all, mapping = aes(x = Commonality, y = Composite.frequency)) +
geom_point() +
geom_line(data = left.points, mapping = aes(x = Commonality, y = Composite.frequency)) +
geom_line(data = right.points, mapping = aes(x = Commonality, y = Composite.frequency))
#Customizing and printing
pw_plot +
theme_bw() +
theme(plot.caption = element_text(hjust = 0)) +
labs(y = "Frequency",
caption = "A piecewise model is fit to the relationship of commonality and total frequency of core words in the combined \ngroup of children. For words below .98 commonality, an exponential model is fit. For words above .98 \ncommonality, the average frequency is displayed.")
row.names(core_words_check_noblanks)[which(prop_4_6 < (.05/length(row.names(common_compare))) )]
core_words_check_noblanks
#creating a smoothing spline with 7 degrees of freedom
smooth.model <- smooth.spline(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency, df = 7)
#Plotting using plot function
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(smooth.model)
#Turning output of smoothing spline into a data frame
smoothing_dataframe <- data.frame(predict(smooth.model))
#Creating the basic ggplot
smoothing_plot <- ggplot(data = ordered_core_words_all, mapping = aes(x = Commonality, y = Composite.frequency)) +
geom_point() +
geom_line(data = smoothing_dataframe, mapping = aes(x = x, y = y))
#Customizing and printing
smoothing_plot +
theme_bw() +
theme(plot.caption = element_text(hjust = 0)) +
labs(y = "Frequency",
caption = "A smoothing spline is fit to the relationship of commonality and total frequency of core words in the combined \ngroup of children.")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tm) #for text to word reader
library(gt) #for nice tables
library(tidyverse)
#List out all files in folder
list_of_files <- list.files(path = "~/2023 Fall/Consulting class/Aided_AAC_Analysis/T4", recursive = TRUE,
pattern = "\\.txt$",
full.names = TRUE)
#Total number of files
num_files <- length(list_of_files)
#First file in Folder: Create a named vector of frequencies
corpus <- Corpus(VectorSource(readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T4/T4101.txt")))
chinese_text <- readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T4/T4101.txt", encoding = "UTF-8")
corpus <- tm_map(corpus, content_transformer(tolower))
dtm <- DocumentTermMatrix(corpus)
word_freq <- colSums(as.matrix(dtm))
total_list <- list(word_freq)
#Do the same for other files
for (jj in 2:num_files) {
#Read in jj file
corpus2 <- Corpus(VectorSource(readLines(list_of_files[jj])))
chinese_text2 <- readLines(list_of_files[jj], encoding = "UTF-8")
corpus2 <- tm_map(corpus2, content_transformer(tolower))
dtm2 <- DocumentTermMatrix(corpus2)
word_freq2 <- colSums(as.matrix(dtm2))
#Combine
total_list[[jj]] <- word_freq2
}
#Extract all names
all_names <- names(unlist(total_list[1]))
for (ii in 2:num_files) {
cur_name <- names(unlist(total_list[ii]))
all_names <- union(all_names, cur_name)}
# Initialize an empty matrix with column names
combined_matrix <- matrix(0, nrow = 0, ncol = length(all_names))
colnames(combined_matrix) <- sort(all_names)
# Iteratively combine vectors into the matrix
for (vec in total_list) {
combined_names <- union(colnames(combined_matrix), names(vec))
combined_matrix <- rbind(combined_matrix, vec[combined_names])
}
# Fill missing values with 0
combined_matrix[is.na(combined_matrix)] <- 0
# Remove English (decided to leave in)
cleaned_freq_4 <- combined_matrix
#List out all files in folder
list_of_files <- list.files(path = "~/2023 Fall/Consulting class/Aided_AAC_Analysis/T5", recursive = TRUE, pattern = "\\.txt$", full.names = TRUE)
#list_of_files <- list.files(path = "https://github.com/smithmor/Aided_AAC_Analysis/tree/d4fb6eef656216d377994c076bd1ecb0278df3c5/T5", recursive = TRUE, pattern = "\\.txt$", full.names = TRUE)
#Some files are corrupted
list_of_files <- list_of_files[-c(8, 12, 24, 25, 43, 53)]
#Total number of files
num_files <- length(list_of_files)
#First file in Folder: Create a named vector of frequencies
#corpus <- Corpus(VectorSource(readLines("https://github.com/smithmor/Aided_AAC_Analysis/blob/d4fb6eef656216d377994c076bd1ecb0278df3c5/T5/T5101.txt")))
#chinese_text <- readLines("https://github.com/smithmor/Aided_AAC_Analysis/tree/d4fb6eef656216d377994c076bd1ecb0278df3c5/T5/T5101.txt", encoding = "UTF-8")
corpus <- Corpus(VectorSource(readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T5/T5101.txt")))
chinese_text <- readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T5/T5101.txt", encoding = "UTF-8")
chinese_text <- chartr("　", " ", chinese_text)
corpus <- tm_map(corpus, content_transformer(tolower))
# Create a custom word tokenizer to split by both space and Unicode 3000
#custom_tokenizer <- function(x) unlist(strsplit(x, split = "[[:space:]]|[\u3000]"))
# Use the custom tokenizer when creating the Document-Term Matrix
#dtm <- DocumentTermMatrix(corpus, control = list(tokenize = custom_tokenizer))
dtm <- DocumentTermMatrix(corpus)
word_freq <- colSums(as.matrix(dtm))
total_list <- list(word_freq)
#Do the same for other files
for (jj in 2:num_files) {
#Read in jj file
#corpus2 <- Corpus(VectorSource(readLines(list_of_files[jj])))
corpus2 <- Corpus(VectorSource(chartr("　", " ", readLines(list_of_files[jj]))))
chinese_text2 <- readLines(list_of_files[jj], encoding = "UTF-8")
chinese_text2 <- chartr("　", " ", chinese_text2)
corpus2 <- tm_map(corpus2, content_transformer(tolower))
# Create a custom word tokenizer to split by both space and Unicode 3000
#custom_tokenizer <- function(x) unlist(strsplit(x, split = "[[:space:]]|[\u3000]"))
# Use the custom tokenizer when creating the Document-Term Matrix
#dtm2 <- DocumentTermMatrix(corpus2, control = list(tokenize = custom_tokenizer))
dtm2 <- DocumentTermMatrix(corpus2)
word_freq2 <- colSums(as.matrix(dtm2))
#Combine
total_list[[jj]] <- word_freq2
}
#Extract all names
all_names <- names(unlist(total_list[1]))
for (ii in 2:num_files) {
cur_name <- names(unlist(total_list[ii]))
all_names <- union(all_names, cur_name)}
# Initialize an empty matrix with column names
combined_matrix <- matrix(0, nrow = 0, ncol = length(all_names))
colnames(combined_matrix) <- sort(all_names)
# Iteratively combine vectors into the matrix
for (vec in total_list) {
combined_names <- union(colnames(combined_matrix), names(vec))
combined_matrix <- rbind(combined_matrix, vec[combined_names])
}
# Fill missing values with 0
combined_matrix[is.na(combined_matrix)] <- 0
# Remove English (decided to leave in)
cleaned_freq_5 <- combined_matrix
#List out all files in folder
list_of_files <- list.files(path = "~/2023 Fall/Consulting class/Aided_AAC_Analysis/T6", recursive = TRUE,
pattern = "\\.txt$",
full.names = TRUE)
#Total number of files
num_files <- length(list_of_files)
#First file in Folder: Create a named vector of frequencies
corpus <- Corpus(VectorSource(chartr("　", " ", readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T6/T6101.txt"))))
chinese_text <- readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T6/T6101.txt", encoding = "UTF-8")
chinese_text <- chartr("　", " ", chinese_text)
corpus <- tm_map(corpus, content_transformer(tolower))
dtm <- DocumentTermMatrix(corpus)
word_freq <- colSums(as.matrix(dtm))
total_list <- list(word_freq)
#Do the same for other files
for (jj in 2:num_files) {
#Read in jj file
corpus2 <- Corpus(VectorSource(chartr("　", " ", readLines(list_of_files[jj]))))
chinese_text2 <- readLines(list_of_files[jj], encoding = "UTF-8")
chinese_text2 <- chartr("　", " ", chinese_text2)
corpus2 <- tm_map(corpus2, content_transformer(tolower))
dtm2 <- DocumentTermMatrix(corpus2)
word_freq2 <- colSums(as.matrix(dtm2))
#Combine
total_list[[jj]] <- word_freq2
}
#Extract all names
all_names <- names(unlist(total_list[1]))
for (ii in 2:num_files) {
cur_name <- names(unlist(total_list[ii]))
all_names <- union(all_names, cur_name)}
# Initialize an empty matrix with column names
combined_matrix <- matrix(0, nrow = 0, ncol = length(all_names))
colnames(combined_matrix) <- sort(all_names)
# Iteratively combine vectors into the matrix
for (vec in total_list) {
combined_names <- union(colnames(combined_matrix), names(vec))
combined_matrix <- rbind(combined_matrix, vec[combined_names])
}
# Fill missing values with 0
combined_matrix[is.na(combined_matrix)] <- 0
# Remove English (decided to leave in)
cleaned_freq_6 <- combined_matrix
#List out all files in folder
list_of_files <- list.files(path = "~/2023 Fall/Consulting class/Aided_AAC_Analysis/all", recursive = TRUE,
pattern = "\\.txt$",
full.names = TRUE)
#Some files are corrupted
list_of_files <- list_of_files[-(c(8, 12, 24, 25, 43, 53) + 40)]
#Total number of files
num_files <- length(list_of_files)
#First file in Folder: Create a named vector of frequencies
corpus <- Corpus(VectorSource(chartr("　", " ", readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/all/T4101.txt"))))
chinese_text <- readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/all/T4101.txt", encoding = "UTF-8")
chinese_text <- chartr("　", " ", chinese_text)
corpus <- tm_map(corpus, content_transformer(tolower))
dtm <- DocumentTermMatrix(corpus)
word_freq <- colSums(as.matrix(dtm))
total_list <- list(word_freq)
#Do the same for other files
for (jj in 2:num_files) {
#Read in jj file
corpus2 <- Corpus(VectorSource(chartr("　", " ", readLines(list_of_files[jj]))))
chinese_text2 <- readLines(list_of_files[jj], encoding = "UTF-8")
chinese_text2 <- chartr("　", " ", chinese_text2)
corpus2 <- tm_map(corpus2, content_transformer(tolower))
dtm2 <- DocumentTermMatrix(corpus2)
word_freq2 <- colSums(as.matrix(dtm2))
#Combine
total_list[[jj]] <- word_freq2
}
#Extract all names
all_names <- names(unlist(total_list[1]))
for (ii in 2:num_files) {
cur_name <- names(unlist(total_list[ii]))
all_names <- union(all_names, cur_name)}
# Initialize an empty matrix with column names
combined_matrix <- matrix(0, nrow = 0, ncol = length(all_names))
colnames(combined_matrix) <- sort(all_names)
# Iteratively combine vectors into the matrix
for (vec in total_list) {
combined_names <- union(colnames(combined_matrix), names(vec))
combined_matrix <- rbind(combined_matrix, vec[combined_names])
}
# Fill missing values with 0
combined_matrix[is.na(combined_matrix)] <- 0
# Remove English (decided to leave in)
cleaned_freq_all <- combined_matrix
#Level of frequency and commonality
freq_level <- .0005
common_level <- .3
#Number of 4 year olds
num_4 <- length(cleaned_freq_4[ ,1])
#Number of total words
total_words_4 <- sum(colSums(cleaned_freq_4))
#Frequent words only
freq_words_4 <- cleaned_freq_4[ , colSums(cleaned_freq_4) > (sum(colSums(cleaned_freq_4))*freq_level)]
#Common words only
common_words_4 <- cleaned_freq_4[ , colSums(1*(cleaned_freq_4 > 0)) > (num_4*common_level)]
#Frequent and common words only (Core words)
core_words_4 <- cleaned_freq_4[ , (colSums(1*(cleaned_freq_4 > 0)) > (num_4*common_level)) & (colSums(cleaned_freq_4) > (sum(colSums(cleaned_freq_4))*freq_level))]
#Turn core words into a dataframe including commonality and frequency
core_words_4_chart <- data.frame('Composite frequency' = colSums(core_words_4), 'Relative frequency per 1000 words' =  colSums(core_words_4)/total_words_4*1000, 'Commonality' = colSums(1*(core_words_4 > 0))/num_4)
#Order by frequency
ordered_core_words_4 <- core_words_4_chart[order(core_words_4_chart[ ,1], decreasing = TRUE), ]
#Level of frequency and commonality
freq_level <- .0005
common_level <- .3
#Number of 5 year olds
num_5 <- length(cleaned_freq_5[ ,1])
#Number of total words
total_words_5 <- sum(colSums(cleaned_freq_5))
#Frequent words only
freq_words_5 <- cleaned_freq_5[ , colSums(cleaned_freq_5) > (sum(colSums(cleaned_freq_5))*freq_level)]
#Common words only
common_words_5 <- cleaned_freq_5[ , colSums(1*(cleaned_freq_5 > 0)) > (num_5*common_level)]
#Frequent and common words only (Core words)
core_words_5 <- cleaned_freq_5[ , (colSums(1*(cleaned_freq_5 > 0)) > (num_5*common_level)) & (colSums(cleaned_freq_5) > (sum(colSums(cleaned_freq_5))*freq_level))]
#Turn core words into a dataframe including commonality and frequency
core_words_5_chart <- data.frame('Composite frequency' = colSums(core_words_5), 'Relative frequency per 1000 words' =  colSums(core_words_5)/total_words_5*1000, 'Commonality' = colSums(1*(core_words_5 > 0))/num_5)
#Order by frequency
ordered_core_words_5 <- core_words_5_chart[order(core_words_5_chart[ ,1], decreasing = TRUE), ]
#Level of frequency and commonality
freq_level <- .0005
common_level <- .3
#Number of 6 year olds
num_6 <- length(cleaned_freq_6[ ,1])
#Number of total words
total_words_6 <- sum(colSums(cleaned_freq_6))
#Frequent words only
freq_words_6 <- cleaned_freq_6[ , colSums(cleaned_freq_6) > (sum(colSums(cleaned_freq_6))*freq_level)]
#Common words only
common_words_6 <- cleaned_freq_6[ , colSums(1*(cleaned_freq_6 > 0)) > (num_6*common_level)]
#Frequent and common words only (Core words)
core_words_6 <- cleaned_freq_6[ , (colSums(1*(cleaned_freq_6 > 0)) > (num_6*common_level)) & (colSums(cleaned_freq_6) > (sum(colSums(cleaned_freq_6))*freq_level))]
#Turn core words into a dataframe including commonality and frequency
core_words_6_chart <- data.frame('Composite frequency' = colSums(core_words_6), 'Relative frequency per 1000 words' =  colSums(core_words_6)/total_words_6*1000, 'Commonality' = colSums(1*(core_words_6 > 0))/num_6)
#Order by frequency
ordered_core_words_6 <- core_words_6_chart[order(core_words_6_chart[ ,1], decreasing = TRUE), ]
#Level of frequency and commonality
freq_level <- .0005
common_level <- .3
#Number of children
num_all <- length(cleaned_freq_all[ ,1])
#Number of total words
total_words_all <- sum(colSums(cleaned_freq_all))
#Frequent words only
freq_words_all <- cleaned_freq_all[ , colSums(cleaned_freq_all) > (sum(colSums(cleaned_freq_all))*freq_level)]
#Common words only
common_words_all <- cleaned_freq_all[ , colSums(1*(cleaned_freq_all > 0)) > (num_all*common_level)]
#Frequent and common words only (Core words)
core_words_all <- cleaned_freq_all[ , (colSums(1*(cleaned_freq_all > 0)) > (num_all*common_level)) & (colSums(cleaned_freq_all) > (sum(colSums(cleaned_freq_all))*freq_level))]
#Turn core words into a dataframe including commonality and frequency
core_words_all_chart <- data.frame('Composite frequency' = colSums(core_words_all), 'Relative frequency per 1000 words' =  colSums(core_words_all)/total_words_all*1000, 'Commonality' = colSums(1*(core_words_all > 0))/num_all)
#Order by frequency
ordered_core_words_all <- core_words_all_chart[order(core_words_all_chart[ ,1], decreasing = TRUE), ]
gtall <- gt(round(ordered_core_words_all[1:6, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gtall, title = "Preview of Core Words Across All Ages")
gt4 <- gt(round(ordered_core_words_4[1:3, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gt4, title = "Preview of Core Words for 4-Year-Olds")
gt5 <- gt(round(ordered_core_words_5[1:3, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gt5, title = "Preview of Core Words for 5-Year-Olds")
gt6 <- gt(round(ordered_core_words_6[1:3, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gt6, title = "Preview of Core Words for 6-Year-Olds")
#creating a smoothing spline with 7 degrees of freedom
smooth.model <- smooth.spline(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency, df = 7)
#Plotting using plot function
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(smooth.model)
#Turning output of smoothing spline into a data frame
smoothing_dataframe <- data.frame(predict(smooth.model))
#Creating the basic ggplot
smoothing_plot <- ggplot(data = ordered_core_words_all, mapping = aes(x = Commonality, y = Composite.frequency)) +
geom_point() +
geom_line(data = smoothing_dataframe, mapping = aes(x = x, y = y))
#Customizing and printing
smoothing_plot +
theme_bw() +
theme(plot.caption = element_text(hjust = 0)) +
labs(y = "Frequency",
caption = "A smoothing spline is fit to the relationship of commonality and total frequency of core words in the combined \ngroup of children.")
#Creating a table with notice of core words & commonality for later
length_of_vects <- length(colnames(cleaned_freq_all))
vec_core_4 <- vector(mode = "character", length = length_of_vects)
vec_core_5 <- vector(mode = "character", length = length_of_vects)
vec_core_6 <- vector(mode = "character", length = length_of_vects)
running_count <- vector(mode = "numeric", length = length_of_vects)
ii <- 0
for (name in colnames(cleaned_freq_all)) {
ii <- ii + 1
ifelse(any(row.names(core_words_4_chart) == name), vec_core_4[ii] <- "x", vec_core_4[ii] <- "")
ifelse(any(row.names(core_words_5_chart) == name), vec_core_5[ii] <- "x", vec_core_5[ii] <- "")
ifelse(any(row.names(core_words_6_chart) == name), vec_core_6[ii] <- "x", vec_core_6[ii] <- "")
}
total_frequency <- colSums(cleaned_freq_all)
total_commonality <- colSums(1*(cleaned_freq_all > 0))
core_words_check <- data.frame('4 Year Olds' = vec_core_4, '5 Year Olds' = vec_core_5, '6 Year Olds' = vec_core_6, 'Composite frequency' = colSums(cleaned_freq_all), 'Relative frequency per 1000 words' =  colSums(cleaned_freq_all)/(sum(colSums(cleaned_freq_all)))*1000, 'Commonality' = colSums(1*(cleaned_freq_all > 0))/(num_all))
row.names(core_words_check) <- colnames(cleaned_freq_all)
core_words_check <- core_words_check[order(core_words_check$Composite.frequency, decreasing = TRUE), ]
check_if_any_core <- function(x){
any(x == "x")
}
core_words_check_noblanks <- core_words_check[apply(core_words_check, MARGIN = 1, check_if_any_core), ]
gtcore <- gt(cbind(core_words_check_noblanks[1:6,1:3 ],round(core_words_check_noblanks[1:6, 4:6], digits = 2)), rownames_to_stub = TRUE)
tab_header(gtcore, title = "Preview of Core Words for Different Age Groups")
#Smoothing the scatterplot
#Averaging
averaged_common <- aggregate(Composite.frequency~Commonality, data = ordered_core_words_all, FUN = mean)
#Simple plot
#plot(averaged_common$Commonality, averaged_common$Composite.frequency)
#Creating ggplot
averaged_scatter_plot <- ggplot(data = averaged_common, mapping = aes(x = Commonality, y = Composite.frequency)) +
geom_point()
#Customizing and printing
averaged_scatter_plot +
theme_bw() +
theme(plot.caption = element_text(hjust = 0)) +
labs(y = "Frequency",
caption = "A smoothed scatterplot created by averaging the frequency of core words with the same commonality in the \ncombined group of children.")
#Dataframe with additional transformed variables
expanded_chart <- cbind(core_words_all_chart, log.freq = log(core_words_all_chart[ ,1]), com2 <- (core_words_all_chart[ ,3])^2, com3 <- (core_words_all_chart[ ,3])^3, com4 <- (core_words_all_chart[ ,3])^4)
#exponential model
exp.model <- lm(log.freq ~ Commonality, data = expanded_chart)
#Points on exponential model
exp.points <- data.frame(Commonality = seq(.3, 1, by = .05), Composite.frequency = exp(exp.model$coefficients[1]*seq(.3, 1, by = .05) + exp.model$coefficients[2]))
#simple plot
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(seq(.3, 1, by=.05), exp(exp.model$coefficients[1]*seq(.3, 1, by=.05) + exp.model$coefficients[2]))
#Creating ggplot
exp_plot <- ggplot(data = ordered_core_words_all, mapping = aes(x = Commonality, y = Composite.frequency)) +
geom_point() +
geom_line(data = exp.points, mapping = aes(x = Commonality, y = Composite.frequency))
#Customizing and printing
exp_plot +
theme_bw() +
theme(plot.caption = element_text(hjust = 0)) +
labs(y = "Frequency",
caption = "An exponential model is fit to the relationship of commonality and total frequency of core words in the combined \ngroup of children.")
#best determined threshold for very common words
chop.point <- .98
#splitting of data
small_comm <- ordered_core_words_all[ordered_core_words_all[,3] < chop.point, ]
small_comm_log <- cbind(small_comm, log.freq = log(small_comm$Composite.frequency))
large_comm <- ordered_core_words_all[ordered_core_words_all[,3] >= chop.point, ]
#fitting of each line
lower.line <- lm(log.freq ~ Commonality, data = small_comm_log)
upper.point <- mean(large_comm$Composite.frequency)
#calculation of total squared loss
sqloss <- sum((small_comm$Composite.frequency - exp(lower.line$coefficients[1]*small_comm$Commonality + lower.line$coefficients[2]))^2) + sum((large_comm$Composite.frequency - upper.point)^2)
#simple plot
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(seq(.3, chop.point, by=.01), exp(lower.line$coefficients[1]*seq(.3, chop.point, by=.01) + lower.line$coefficients[2]))
#lines(seq(chop.point, 1, by=.01), seq(chop.point, 1, by=.01)*0+upper.point)
#Points on exponential model
left.points <- data.frame(Commonality = seq(.3, chop.point, by = .01), Composite.frequency = exp(lower.line$coefficients[1]*seq(.3, chop.point, by = .01) + lower.line$coefficients[2]))
right.points <- data.frame(Commonality = seq(chop.point, 1, by = .01), Composite.frequency = seq(chop.point, 1, by = .01)*0 + upper.point)
#simple plot
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(seq(.3, 1, by=.05), exp(exp.model$coefficients[1]*seq(.3, 1, by=.05) + exp.model$coefficients[2]))
#Creating ggplot
pw_plot <- ggplot(data = ordered_core_words_all, mapping = aes(x = Commonality, y = Composite.frequency)) +
geom_point() +
geom_line(data = left.points, mapping = aes(x = Commonality, y = Composite.frequency)) +
geom_line(data = right.points, mapping = aes(x = Commonality, y = Composite.frequency))
#Customizing and printing
pw_plot +
theme_bw() +
theme(plot.caption = element_text(hjust = 0)) +
labs(y = "Frequency",
caption = "A piecewise model is fit to the relationship of commonality and total frequency of core words in the combined \ngroup of children. For words below .98 commonality, an exponential model is fit. For words above .98 \ncommonality, the average frequency is displayed.")
