---
title: "Determining Core Words for Mandarin-Speaking Children"
author: 
  - "Written by: Morgan Wood"
  - "Client: Hsiao-Ting Su"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    number_sections: true
    theme: journal
abstract: |
  This is my abstract paragraph 1. With details.
  
  This is paragraph 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tm) #for text to word reader 
library(gt) #for nice tables
library(tidyverse)
```

```{r 4yr, include=FALSE}
#List out all files in folder

list_of_files <- list.files(path = "~/2023 Fall/Consulting class/Aided_AAC_Analysis/T4", recursive = TRUE,
                            pattern = "\\.txt$", 
                            full.names = TRUE)



#Total number of files

num_files <- length(list_of_files)



#First file in Folder: Create a named vector of frequencies

corpus <- Corpus(VectorSource(readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T4/T4101.txt")))

chinese_text <- readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T4/T4101.txt", encoding = "UTF-8")

corpus <- tm_map(corpus, content_transformer(tolower))

dtm <- DocumentTermMatrix(corpus)

word_freq <- colSums(as.matrix(dtm))

total_list <- list(word_freq)



#Do the same for other files
for (jj in 2:num_files) {
  
  #Read in jj file
  corpus2 <- Corpus(VectorSource(readLines(list_of_files[jj])))

  chinese_text2 <- readLines(list_of_files[jj], encoding = "UTF-8")

  corpus2 <- tm_map(corpus2, content_transformer(tolower))

  dtm2 <- DocumentTermMatrix(corpus2)

  word_freq2 <- colSums(as.matrix(dtm2))
  
  #Combine
  total_list[[jj]] <- word_freq2
  
}


#Extract all names
all_names <- names(unlist(total_list[1]))
for (ii in 2:num_files) { 
  cur_name <- names(unlist(total_list[ii]))
  all_names <- union(all_names, cur_name)}

# Initialize an empty matrix with column names
combined_matrix <- matrix(0, nrow = 0, ncol = length(all_names))
colnames(combined_matrix) <- sort(all_names)

# Iteratively combine vectors into the matrix
for (vec in total_list) {
  combined_names <- union(colnames(combined_matrix), names(vec))
  combined_matrix <- rbind(combined_matrix, vec[combined_names])
}



# Fill missing values with 0
combined_matrix[is.na(combined_matrix)] <- 0



# Remove English (decided to leave in)
cleaned_freq_4 <- combined_matrix
```

```{r 5yr, include=FALSE}
#List out all files in folder

list_of_files <- list.files(path = "~/2023 Fall/Consulting class/Aided_AAC_Analysis/T5", recursive = TRUE, pattern = "\\.txt$", full.names = TRUE)

#list_of_files <- list.files(path = "https://github.com/smithmor/Aided_AAC_Analysis/tree/d4fb6eef656216d377994c076bd1ecb0278df3c5/T5", recursive = TRUE, pattern = "\\.txt$", full.names = TRUE)

#Some files are corrupted
list_of_files <- list_of_files[-c(8, 12, 24, 25, 43, 53)]


#Total number of files

num_files <- length(list_of_files)



#First file in Folder: Create a named vector of frequencies

#corpus <- Corpus(VectorSource(readLines("https://github.com/smithmor/Aided_AAC_Analysis/blob/d4fb6eef656216d377994c076bd1ecb0278df3c5/T5/T5101.txt")))

#chinese_text <- readLines("https://github.com/smithmor/Aided_AAC_Analysis/tree/d4fb6eef656216d377994c076bd1ecb0278df3c5/T5/T5101.txt", encoding = "UTF-8")

corpus <- Corpus(VectorSource(readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T5/T5101.txt")))

chinese_text <- readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T5/T5101.txt", encoding = "UTF-8")

chinese_text <- chartr("　", " ", chinese_text)

corpus <- tm_map(corpus, content_transformer(tolower))

# Create a custom word tokenizer to split by both space and Unicode 3000
#custom_tokenizer <- function(x) unlist(strsplit(x, split = "[[:space:]]|[\u3000]"))

# Use the custom tokenizer when creating the Document-Term Matrix
#dtm <- DocumentTermMatrix(corpus, control = list(tokenize = custom_tokenizer))

dtm <- DocumentTermMatrix(corpus)

word_freq <- colSums(as.matrix(dtm))

total_list <- list(word_freq)



#Do the same for other files
for (jj in 2:num_files) {
  
  #Read in jj file
  #corpus2 <- Corpus(VectorSource(readLines(list_of_files[jj])))
  
  corpus2 <- Corpus(VectorSource(chartr("　", " ", readLines(list_of_files[jj]))))

  chinese_text2 <- readLines(list_of_files[jj], encoding = "UTF-8")
  
  chinese_text2 <- chartr("　", " ", chinese_text2)

  corpus2 <- tm_map(corpus2, content_transformer(tolower))
  
  # Create a custom word tokenizer to split by both space and Unicode 3000
  #custom_tokenizer <- function(x) unlist(strsplit(x, split = "[[:space:]]|[\u3000]"))

  # Use the custom tokenizer when creating the Document-Term Matrix
  #dtm2 <- DocumentTermMatrix(corpus2, control = list(tokenize = custom_tokenizer))

  dtm2 <- DocumentTermMatrix(corpus2)

  word_freq2 <- colSums(as.matrix(dtm2))
  
  #Combine
  total_list[[jj]] <- word_freq2
  
}


#Extract all names
all_names <- names(unlist(total_list[1]))
for (ii in 2:num_files) { 
  cur_name <- names(unlist(total_list[ii]))
  all_names <- union(all_names, cur_name)}

# Initialize an empty matrix with column names
combined_matrix <- matrix(0, nrow = 0, ncol = length(all_names))
colnames(combined_matrix) <- sort(all_names)

# Iteratively combine vectors into the matrix
for (vec in total_list) {
  combined_names <- union(colnames(combined_matrix), names(vec))
  combined_matrix <- rbind(combined_matrix, vec[combined_names])
}



# Fill missing values with 0
combined_matrix[is.na(combined_matrix)] <- 0



# Remove English (decided to leave in)
cleaned_freq_5 <- combined_matrix
```

```{r 6yr read in, include=FALSE}
#List out all files in folder

list_of_files <- list.files(path = "~/2023 Fall/Consulting class/Aided_AAC_Analysis/T6", recursive = TRUE,
                            pattern = "\\.txt$", 
                            full.names = TRUE)


#Total number of files

num_files <- length(list_of_files)



#First file in Folder: Create a named vector of frequencies

corpus <- Corpus(VectorSource(chartr("　", " ", readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T6/T6101.txt"))))

chinese_text <- readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/T6/T6101.txt", encoding = "UTF-8")

chinese_text <- chartr("　", " ", chinese_text)

corpus <- tm_map(corpus, content_transformer(tolower))

dtm <- DocumentTermMatrix(corpus)

word_freq <- colSums(as.matrix(dtm))

total_list <- list(word_freq)



#Do the same for other files
for (jj in 2:num_files) {
  
  #Read in jj file
  corpus2 <- Corpus(VectorSource(chartr("　", " ", readLines(list_of_files[jj]))))

  chinese_text2 <- readLines(list_of_files[jj], encoding = "UTF-8")
  
  chinese_text2 <- chartr("　", " ", chinese_text2)

  corpus2 <- tm_map(corpus2, content_transformer(tolower))

  dtm2 <- DocumentTermMatrix(corpus2)

  word_freq2 <- colSums(as.matrix(dtm2))
  
  #Combine
  total_list[[jj]] <- word_freq2
  
}


#Extract all names
all_names <- names(unlist(total_list[1]))
for (ii in 2:num_files) { 
  cur_name <- names(unlist(total_list[ii]))
  all_names <- union(all_names, cur_name)}

# Initialize an empty matrix with column names
combined_matrix <- matrix(0, nrow = 0, ncol = length(all_names))
colnames(combined_matrix) <- sort(all_names)

# Iteratively combine vectors into the matrix
for (vec in total_list) {
  combined_names <- union(colnames(combined_matrix), names(vec))
  combined_matrix <- rbind(combined_matrix, vec[combined_names])
}



# Fill missing values with 0
combined_matrix[is.na(combined_matrix)] <- 0



# Remove English (decided to leave in)
cleaned_freq_6 <- combined_matrix
```

```{r all_read_in, include=FALSE}
#List out all files in folder

list_of_files <- list.files(path = "~/2023 Fall/Consulting class/Aided_AAC_Analysis/all", recursive = TRUE,
                            pattern = "\\.txt$", 
                            full.names = TRUE)



#Some files are corrupted
list_of_files <- list_of_files[-(c(8, 12, 24, 25, 43, 53) + 40)]



#Total number of files

num_files <- length(list_of_files)



#First file in Folder: Create a named vector of frequencies

corpus <- Corpus(VectorSource(chartr("　", " ", readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/all/T4101.txt"))))

chinese_text <- readLines("~/2023 Fall/Consulting class/Aided_AAC_Analysis/all/T4101.txt", encoding = "UTF-8")

chinese_text <- chartr("　", " ", chinese_text)

corpus <- tm_map(corpus, content_transformer(tolower))

dtm <- DocumentTermMatrix(corpus)

word_freq <- colSums(as.matrix(dtm))

total_list <- list(word_freq)



#Do the same for other files
for (jj in 2:num_files) {
  
  #Read in jj file
  corpus2 <- Corpus(VectorSource(chartr("　", " ", readLines(list_of_files[jj]))))

  chinese_text2 <- readLines(list_of_files[jj], encoding = "UTF-8")
  
  chinese_text2 <- chartr("　", " ", chinese_text2)

  corpus2 <- tm_map(corpus2, content_transformer(tolower))

  dtm2 <- DocumentTermMatrix(corpus2)

  word_freq2 <- colSums(as.matrix(dtm2))
  
  #Combine
  total_list[[jj]] <- word_freq2
  
}


#Extract all names
all_names <- names(unlist(total_list[1]))
for (ii in 2:num_files) { 
  cur_name <- names(unlist(total_list[ii]))
  all_names <- union(all_names, cur_name)}

# Initialize an empty matrix with column names
combined_matrix <- matrix(0, nrow = 0, ncol = length(all_names))
colnames(combined_matrix) <- sort(all_names)

# Iteratively combine vectors into the matrix
for (vec in total_list) {
  combined_names <- union(colnames(combined_matrix), names(vec))
  combined_matrix <- rbind(combined_matrix, vec[combined_names])
}



# Fill missing values with 0
combined_matrix[is.na(combined_matrix)] <- 0



# Remove English (decided to leave in)
cleaned_freq_all <- combined_matrix
```

```{r 4yr core words, include=FALSE}
#Level of frequency and commonality
freq_level <- .0005
common_level <- .3

#Number of 4 year olds
num_4 <- length(cleaned_freq_4[ ,1])

#Number of total words
total_words_4 <- sum(colSums(cleaned_freq_4))

#Frequent words only
freq_words_4 <- cleaned_freq_4[ , colSums(cleaned_freq_4) > (sum(colSums(cleaned_freq_4))*freq_level)]

#Common words only
common_words_4 <- cleaned_freq_4[ , colSums(1*(cleaned_freq_4 > 0)) > (num_4*common_level)]

#Frequent and common words only (Core words)
core_words_4 <- cleaned_freq_4[ , (colSums(1*(cleaned_freq_4 > 0)) > (num_4*common_level)) & (colSums(cleaned_freq_4) > (sum(colSums(cleaned_freq_4))*freq_level))]

#Turn core words into a dataframe including commonality and frequency
core_words_4_chart <- data.frame('Composite frequency' = colSums(core_words_4), 'Relative frequency per 1000 words' =  colSums(core_words_4)/total_words_4*1000, 'Commonality' = colSums(1*(core_words_4 > 0))/num_4)

#Order by frequency
ordered_core_words_4 <- core_words_4_chart[order(core_words_4_chart[ ,1], decreasing = TRUE), ]

```

```{r 5yr core words, include=FALSE}
#Level of frequency and commonality
freq_level <- .0005
common_level <- .3

#Number of 5 year olds
num_5 <- length(cleaned_freq_5[ ,1])

#Number of total words
total_words_5 <- sum(colSums(cleaned_freq_5))

#Frequent words only
freq_words_5 <- cleaned_freq_5[ , colSums(cleaned_freq_5) > (sum(colSums(cleaned_freq_5))*freq_level)]

#Common words only
common_words_5 <- cleaned_freq_5[ , colSums(1*(cleaned_freq_5 > 0)) > (num_5*common_level)]

#Frequent and common words only (Core words)
core_words_5 <- cleaned_freq_5[ , (colSums(1*(cleaned_freq_5 > 0)) > (num_5*common_level)) & (colSums(cleaned_freq_5) > (sum(colSums(cleaned_freq_5))*freq_level))]

#Turn core words into a dataframe including commonality and frequency
core_words_5_chart <- data.frame('Composite frequency' = colSums(core_words_5), 'Relative frequency per 1000 words' =  colSums(core_words_5)/total_words_5*1000, 'Commonality' = colSums(1*(core_words_5 > 0))/num_5)

#Order by frequency
ordered_core_words_5 <- core_words_5_chart[order(core_words_5_chart[ ,1], decreasing = TRUE), ]

```

```{r 6yr core words, include=FALSE}
#Level of frequency and commonality
freq_level <- .0005
common_level <- .3

#Number of 6 year olds
num_6 <- length(cleaned_freq_6[ ,1])

#Number of total words
total_words_6 <- sum(colSums(cleaned_freq_6))

#Frequent words only
freq_words_6 <- cleaned_freq_6[ , colSums(cleaned_freq_6) > (sum(colSums(cleaned_freq_6))*freq_level)]

#Common words only
common_words_6 <- cleaned_freq_6[ , colSums(1*(cleaned_freq_6 > 0)) > (num_6*common_level)]

#Frequent and common words only (Core words)
core_words_6 <- cleaned_freq_6[ , (colSums(1*(cleaned_freq_6 > 0)) > (num_6*common_level)) & (colSums(cleaned_freq_6) > (sum(colSums(cleaned_freq_6))*freq_level))]

#Turn core words into a dataframe including commonality and frequency
core_words_6_chart <- data.frame('Composite frequency' = colSums(core_words_6), 'Relative frequency per 1000 words' =  colSums(core_words_6)/total_words_6*1000, 'Commonality' = colSums(1*(core_words_6 > 0))/num_6)

#Order by frequency
ordered_core_words_6 <- core_words_6_chart[order(core_words_6_chart[ ,1], decreasing = TRUE), ]

```

```{r all years core words, include=FALSE}
#Level of frequency and commonality
freq_level <- .0005
common_level <- .3

#Number of children
num_all <- length(cleaned_freq_all[ ,1])

#Number of total words
total_words_all <- sum(colSums(cleaned_freq_all))

#Frequent words only
freq_words_all <- cleaned_freq_all[ , colSums(cleaned_freq_all) > (sum(colSums(cleaned_freq_all))*freq_level)]

#Common words only
common_words_all <- cleaned_freq_all[ , colSums(1*(cleaned_freq_all > 0)) > (num_all*common_level)]

#Frequent and common words only (Core words)
core_words_all <- cleaned_freq_all[ , (colSums(1*(cleaned_freq_all > 0)) > (num_all*common_level)) & (colSums(cleaned_freq_all) > (sum(colSums(cleaned_freq_all))*freq_level))]

#Turn core words into a dataframe including commonality and frequency
core_words_all_chart <- data.frame('Composite frequency' = colSums(core_words_all), 'Relative frequency per 1000 words' =  colSums(core_words_all)/total_words_all*1000, 'Commonality' = colSums(1*(core_words_all > 0))/num_all)

#Order by frequency
ordered_core_words_all <- core_words_all_chart[order(core_words_all_chart[ ,1], decreasing = TRUE), ]

```


# Overview

Determining core words for a set of individuals is an important first step in developing Aided Augmentative and Alternative Communication devices (Aided AAC) tailored for those individuals. Aided AAC are either electronic or non-electronic devices used as an alternative to traditional verbal communication. Examples of Aided AAC are communication boards or speech generating devices.

The goal of this report is to display a set of potential core words for Mandarin speaking children between the ages of 4 and 6 to aid in the creation of Aided AAC for these individuals.

To determine core words, multiple text files containing transcribed stories told by children are deconstructed into a list of words. In our setting, a word can by a single Chinese character or multiple Chinese characters. 

For this report, I consider four different groups of children each with $n$ children within the group and a total number of $w$ words spoken. The groups considered are listed below.

* All Children (Ages 4 through 6) ($n$ = `r num_all`, $w$ = `r format(total_words_all, scientific = FALSE)`)

* Children of Age 4 ($n$ = `r num_4`, $w$ = `r format(total_words_4, scientific = FALSE)`)

* Children of Age 5 ($n$ = `r num_5`, $w$ = `r format(total_words_5, scientific = FALSE)`)

* Children of Age 6 ($n$ = `r num_6`, $w$ = `r format(total_words_6, scientific = FALSE)`)

For each group of children, two statistics are computed for each distinct word spoken. First, if a word is spoken by $c\cdot100\%$ of children in the group, the word is assigned a commonality $c$. The frequency that a word is spoken is also looked at. From this we can obtain a relative frequency $r$ which will be the number of times per 1000 words the word is spoken. The total frequency of each word is also reported.

Within this report, a word is considered core if at least 30% of children spoke the word and the word is spoken with a relative frequency of at least 0.5 times per 1000 words. I refer to .3 as the commonality threshold and .5 as the frequency threshold.

# Choice of Commonality and Frequency Threshold

I begin this report with a comment on the choice of threshold for declaring a core word. 

While commonality and frequency represent two different statistics of the data, they are related. For example, if a word has commonality $1$, then this requires that each individual in the study spoke the word. This implies that the word is spoke with a cumulative frequency at least equal to the number of individuals $n$. More generally, if a word has a commonality of $c$, the relative frequency must be at least 

$$1000 \cdot\frac{c\cdot n}{w}$$
where $w$ is the total number of words spoken.

In fact, if we require a commonality threshold of $c=0.3$, this forces a relative frequency $r$ of at least the following for each group.

* All Children (Ages 4 through 6): $r \geq$ `r 300*num_all/total_words_all`

* Children of Age 4: $r \geq$ `r 300*num_4/total_words_4`

* Children of Age 5: $r \geq$ `r 300*num_5/total_words_5`

* Children of Age 6: $r \geq$ `r 300*num_6/total_words_6`

Thus, unless a frequency threshold is above the number above, this threshold will not play a role in choosing which words are considered core. If it is desired for frequency to play a role in selection core words, I suggest selecting a frequency threshold strictly larger than the values listed above.

Regardless of this, for the remainder of this report I continue with the commonality and frequency thresholds $c=.3$ and $r=.5$, respectively, because this was requested. This implies that core words were actually only chosen with the condition that the commonality was at least 30%.

# Core Words

Below, I give a table that previews the first 6 core words for children in the combined group. The core words listed below are in order of decreasing frequency. In total, there were `r length(ordered_core_words_all[ , 1])` core words identified. The complete list of core words can be found on GitHub in the excel file "core_words_processed.xlsx"^[Link: https://github.com/smithmor/Aided_AAC_Analysis/blob/7cab44371c38a43652a1a81eacb7862b4a6128c9/core_words_processed.xlsx] under the "Core Words for Combined Group" tab.

```{r, echo=FALSE}
gtall <- gt(round(ordered_core_words_all[1:6, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gtall, title = "Preview of Core Words Across All Ages")
```
Similarly, core words for other groups can be found. Below is a preview of core words for 4-year-olds, 5-year-olds, and 6-year-olds, which had a total of `r length(ordered_core_words_4[ , 1])`, `r length(ordered_core_words_5[ , 1])`, and `r length(ordered_core_words_6[ , 1])` core words identified, respectively. It is worth noting that the number of core words increase with each age group. This matches the intuition that older children will have a broader vocabulary. Also of interest is the observation that one child in the 6-year-old group did not use the word "`r row.names(ordered_core_words_6)[1]`" which corresponds to the English word "I".

```{r, echo=FALSE}
gt4 <- gt(round(ordered_core_words_4[1:3, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gt4, title = "Preview of Core Words for 4-Year-Olds")

gt5 <- gt(round(ordered_core_words_5[1:3, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gt5, title = "Preview of Core Words for 5-Year-Olds")

gt6 <- gt(round(ordered_core_words_6[1:3, ], digits = 2), rownames_to_stub = TRUE)
tab_header(gt6, title = "Preview of Core Words for 6-Year-Olds")
```
The complete list of each of these charts can be found also in the excel file "core_words_processed.xlsx" under the corresponding "Core Words for X-Year-Olds" tab.


# Relationship Between Commonality and Frequency

Next, I explore the relationship between commonality and frequency. To provide one possible summary of this relationship, a smoothing spline was fit to the set of core words (as defined by the combined group). This can be found below. As expected, we see a generally increasing relationship. Graphically, we also see increased variability in the relationship as the commonality increases.

```{r, echo=FALSE, fig.align = 'center'}
#creating a smoothing spline with 7 degrees of freedom
smooth.model <- smooth.spline(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency, df = 7)

#Plotting using plot function
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(smooth.model)

#Turning output of smoothing spline into a data frame
smoothing_dataframe <- data.frame(predict(smooth.model))

#Creating the basic ggplot
smoothing_plot <- ggplot(data = ordered_core_words_all, mapping = aes(x = Commonality, y = Composite.frequency)) +
  geom_point() +
  geom_line(data = smoothing_dataframe, mapping = aes(x = x, y = y))

#Customizing and printing
smoothing_plot + 
  theme_bw() +
  theme(plot.caption = element_text(hjust = 0)) +
  labs(y = "Frequency",
       caption = "A smoothing spline is fit to the relationship of commonality and total frequency of core words in the combined \ngroup of children.")
  
```

The plot above can be used to summarize the relationship graphically as well as to predict the frequency of a core word given its commonality.

Smoothing splines require choosing a parameter that controls the "wiggliness" of the line. Here, I chose a parameter by choosing the curve that graphically best fit the data without appearing to overfit. However, the smoothing parameter can also be chosen using cross validation and results in a "wigglier" line that using intuition appears to me to suffer from over-fitting.

Other potential models are explored in the Appendix. Namely, an exponential model is fit as well as a discontinuous piecewise function. These models have the advantage of having a fitted curve with a closed-form equation. However, they do not fit the shape of the data as well as the smoothing spline displayed above. 

Alternatively, a smoothed scatterplot is also presented in the Appendix.

# Core Words Between Different Age Groups

Next, we look to see if the commonality of possible core words change between different age groups. To do this, we consider all words that are determined to be a core word in the 4-year-old, 5-year-old, or 6-year-old group. Below is a preview of a chart which catalogs the groups in which each word is classified as a core word. The complete charts can be found in the excel file "core_words_processed.xlsx" under the "Core Words Comparison" tab.

```{r, include=FALSE}
#Creating a table with notice of core words & commonality for later
length_of_vects <- length(colnames(cleaned_freq_all))

vec_core_4 <- vector(mode = "character", length = length_of_vects)
vec_core_5 <- vector(mode = "character", length = length_of_vects)
vec_core_6 <- vector(mode = "character", length = length_of_vects)
running_count <- vector(mode = "numeric", length = length_of_vects)
  
ii <- 0

for (name in colnames(cleaned_freq_all)) {
  ii <- ii + 1
  ifelse(any(row.names(core_words_4_chart) == name), vec_core_4[ii] <- "x", vec_core_4[ii] <- "")
  ifelse(any(row.names(core_words_5_chart) == name), vec_core_5[ii] <- "x", vec_core_5[ii] <- "")
  ifelse(any(row.names(core_words_6_chart) == name), vec_core_6[ii] <- "x", vec_core_6[ii] <- "")
}

total_frequency <- colSums(cleaned_freq_all)

total_commonality <- colSums(1*(cleaned_freq_all > 0))

core_words_check <- data.frame('4 Year Olds' = vec_core_4, '5 Year Olds' = vec_core_5, '6 Year Olds' = vec_core_6, 'Composite frequency' = colSums(cleaned_freq_all), 'Relative frequency per 1000 words' =  colSums(cleaned_freq_all)/(sum(colSums(cleaned_freq_all)))*1000, 'Commonality' = colSums(1*(cleaned_freq_all > 0))/(num_all))

row.names(core_words_check) <- colnames(cleaned_freq_all)

core_words_check <- core_words_check[order(core_words_check$Composite.frequency, decreasing = TRUE), ]

check_if_any_core <- function(x){
  any(x == "x")
}

core_words_check_noblanks <- core_words_check[apply(core_words_check, MARGIN = 1, check_if_any_core), ]
```

```{r, echo=FALSE}
gtcore <- gt(cbind(core_words_check_noblanks[1:6,1:3 ],round(core_words_check_noblanks[1:6, 4:6], digits = 2)), rownames_to_stub = TRUE)
tab_header(gtcore, title = "Preview of Core Words for Different Age Groups")
```
In the above chart, if a word is considered core within a certain age group, this is represented with an "x" in the corresponding cell. This chart also includes the frequency and commonality in the combined group and orders each word by decreasing frequency.

For each of the words considered core for any age group, we conduct a statistical test to see if the commonality of the word significantly differs between two different age groups. This is done though a z-test for the difference of proportions (or equivalently a chi-squared test for independence).

After accounting for multiple tests using the Bonferroni correction, we find no significant difference between the commonality of any core word in the age group of 4 verses the age group of 5. Similarly, we find no significant difference between the age group of 5 and the age group of 6.

We do find a significant difference in the commonality of at least one word between the 4-year-old age group and the 6-year-old age group. Specifically, the word 她 is significantly more common in the 6-year-old age group than in the 4-year-old age group with a commonality of 62% verses 13%. This suggests that the core words between these two age groups are significantly different. Thus, there may be an advantage to creating different Aided AAC for 4-year-olds than for 6-year-olds if considering the commonality threshold of 0.3 and frequency threshold of 0.5.

# Appendix: Other Models of the Relationship Between Commonality and Frequency

We begin with an alternative scatterplot representation that smooths the relationship by averaging the frequency of words with the same commonality.

```{r, echo=FALSE, fig.align = 'center'}
#Smoothing the scatterplot

#Averaging
averaged_common <- aggregate(Composite.frequency~Commonality, data = ordered_core_words_all, FUN = mean) 

#Simple plot
#plot(averaged_common$Commonality, averaged_common$Composite.frequency)

#Creating ggplot
averaged_scatter_plot <- ggplot(data = averaged_common, mapping = aes(x = Commonality, y = Composite.frequency)) +
  geom_point()

#Customizing and printing
averaged_scatter_plot + 
  theme_bw() +
  theme(plot.caption = element_text(hjust = 0)) +
  labs(y = "Frequency",
       caption = "A smoothed scatterplot created by averaging the frequency of core words with the same commonality in the \ncombined group of children.")
```

In addition to the smoothing spline presented in the main report, we also explored other models for the relationship between commonality and frequency. Below are plots corresponding to two other potential models followed by a brief discussion on each model.

We first consider an exponential model. One advantage to this model is that the line of best fit can be expressed in a closed-form expression. In fact, fitting an exponential curve to the relationship between commonality and frequency we see can obtain the prediction

$$\text{Estimated Frequency} = 50.46\cdot 18.88^{c}$$
for a word with commonality $c$. This curve is displayed below. Unfortunately, this curve, while easy to interpret, does not fit the shape of the relationship well.


```{r, echo=FALSE, fig.align = 'center'}
#Dataframe with additional transformed variables
expanded_chart <- cbind(core_words_all_chart, log.freq = log(core_words_all_chart[ ,1]), com2 <- (core_words_all_chart[ ,3])^2, com3 <- (core_words_all_chart[ ,3])^3, com4 <- (core_words_all_chart[ ,3])^4)

#exponential model
exp.model <- lm(log.freq ~ Commonality, data = expanded_chart)

#Points on exponential model
exp.points <- data.frame(Commonality = seq(.3, 1, by = .05), Composite.frequency = exp(exp.model$coefficients[1]*seq(.3, 1, by = .05) + exp.model$coefficients[2]))

#simple plot
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(seq(.3, 1, by=.05), exp(exp.model$coefficients[1]*seq(.3, 1, by=.05) + exp.model$coefficients[2]))

#Creating ggplot
exp_plot <- ggplot(data = ordered_core_words_all, mapping = aes(x = Commonality, y = Composite.frequency)) +
  geom_point() +
  geom_line(data = exp.points, mapping = aes(x = Commonality, y = Composite.frequency))

#Customizing and printing
exp_plot + 
  theme_bw() +
  theme(plot.caption = element_text(hjust = 0)) +
  labs(y = "Frequency",
       caption = "An exponential model is fit to the relationship of commonality and total frequency of core words in the combined \ngroup of children.")
```

We next considered a piece-wise smooth model based on intuition. It is possible that words can be described as either "very common" or "not very common". The very common words can be thought of as the English equivalent of "I", "is", etc. The very common words correspond to the words on the far right-hand side of the scatterplots.

These very common words appear to have a different distribution than the other words. For example, the variance in the very common words is much higher. There is also much less of a clear pattern. One possible hypothesis is that not very common words fit an exponential distribution while very common words are randomly distributed around some common average frequency. 

To fit a model using the above description, we must determine (1) a threshold for words being classified as very common, (2) an exponential model for the not very common words, and (3) an average frequency for very common words.

Below is the curve that minimizes squared error which uses a threshold of 0.98 commonality. This model has the same benefit as the exponential model in that the relationship can be expressed in closed-form, while having a much closer fit to the true data.

$$\text{Estimated Frequency} = \begin{cases} 36.22\cdot 22.02^{c} & \text{if } c\leq .98,\\  1682.57 & \text{if } c> .98, \end{cases}$$
for a word with commonality $c$. 

```{r, echo=FALSE, fig.align = 'center'}
#best determined threshold for very common words
chop.point <- .98

#splitting of data
small_comm <- ordered_core_words_all[ordered_core_words_all[,3] < chop.point, ]
small_comm_log <- cbind(small_comm, log.freq = log(small_comm$Composite.frequency))
large_comm <- ordered_core_words_all[ordered_core_words_all[,3] >= chop.point, ]

#fitting of each line
lower.line <- lm(log.freq ~ Commonality, data = small_comm_log)
upper.point <- mean(large_comm$Composite.frequency)

#calculation of total squared loss
sqloss <- sum((small_comm$Composite.frequency - exp(lower.line$coefficients[1]*small_comm$Commonality + lower.line$coefficients[2]))^2) + sum((large_comm$Composite.frequency - upper.point)^2)

#simple plot
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(seq(.3, chop.point, by=.01), exp(lower.line$coefficients[1]*seq(.3, chop.point, by=.01) + lower.line$coefficients[2]))
#lines(seq(chop.point, 1, by=.01), seq(chop.point, 1, by=.01)*0+upper.point)

#Points on exponential model
left.points <- data.frame(Commonality = seq(.3, chop.point, by = .01), Composite.frequency = exp(lower.line$coefficients[1]*seq(.3, chop.point, by = .01) + lower.line$coefficients[2]))
right.points <- data.frame(Commonality = seq(chop.point, 1, by = .01), Composite.frequency = seq(chop.point, 1, by = .01)*0 + upper.point)

#simple plot
#plot(ordered_core_words_all$Commonality, ordered_core_words_all$Composite.frequency)
#lines(seq(.3, 1, by=.05), exp(exp.model$coefficients[1]*seq(.3, 1, by=.05) + exp.model$coefficients[2]))

#Creating ggplot
pw_plot <- ggplot(data = ordered_core_words_all, mapping = aes(x = Commonality, y = Composite.frequency)) +
  geom_point() +
  geom_line(data = left.points, mapping = aes(x = Commonality, y = Composite.frequency)) +
  geom_line(data = right.points, mapping = aes(x = Commonality, y = Composite.frequency))

#Customizing and printing
pw_plot + 
  theme_bw() +
  theme(plot.caption = element_text(hjust = 0)) +
  labs(y = "Frequency",
       caption = "A piecewise model is fit to the relationship of commonality and total frequency of core words in the combined \ngroup of children. For words below .98 commonality, an exponential model is fit. For words above .98 \ncommonality, the average frequency is displayed.")
```

One potential drawback to the above curve is that the model performance is very sensitive to the choice of threshold. Also, words that have commonality either just to the left or the threshold or just to the right of the threshold will have drastically different estimates of frequency which is not ideal.

Because of the drawbacks of the above models, I believe the smoothing spline given in the main report is a better choice for representing the relationship between commality and frequency.
